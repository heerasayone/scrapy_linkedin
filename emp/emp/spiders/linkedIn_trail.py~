import scrapy
import re
import csv,math
import requests
import time
import json
from lxml import html
from scrapy.http import Request, FormRequest
from ..import items
from pkg_resources import resource_stream
from selenium import webdriver
import random
import urllib2
# import requests

include_package_data = True

# header = {'Referer':''}


class Employee_details(scrapy.Spider):
    name = "linkedin1"
    domain = "https://www.linkedin.com"
    start_urls = ["https://www.linkedin.com"]
    # company_search_url = 'https://www.linkedin.com/vsearch/c?type=companies&keywords=BAKERSFIELD+PIPE+%26+SUPPLY+INC&'
    company_search_url ="https://www.linkedin.com/vsearch/c?type=companies&keywords={company_name}&"
    i = 0
    ss = 0
    # download_delay = 5

    fieldnames = ['Name', 'Designation', 'Address', 'Linkedin_id', 'Company_name', 'Given_street', 'Given_city']

#     headers = {
#       'host':'www.linkedin.com',
#       'method':'GET',
#       'path':'/vsearch/c?type=companies&keywords=legal_name&',
#       'scheme':'https',
#       'version:HTTP/1.1
# accept:text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8
# accept-encoding:gzip,deflate,sdch
# accept-language:en-US,en;q=0.8
# cache-control:max-age=0
#     }

    def excecute_random_event_matrix(self):
      self.driver = webdriver.Firefox()
      url = "https://www.linkedin.com/"
      self.driver.get(url)
      event1 = self.driver.find_element_by_xpath('.//div[@class="link"]//ul//li/a')
      event2 = self.driver.find_element_by_xpath('//ul[@class="nav-legal"]//li//a')
      event3 = self.driver.find_element_by_xpath('//li[@class="lang-container"]//a')
      event4 = self.driver.find_element_by_xpath('//li[@class="first"]//a')
      events = [event1, event2, event3, event4]
      random_event = random.choice(events)
      random_event.click()
      self.driver.close()
#
# formdata={'session_key': 'inside.out4001@gmail.com', 'session_password': 'insideout4001',
#                                   'csrfToken': csrftoken},

    # def process_reqeust(self, reqeust):
    #
    #     http_proxy = "http://188.211.26.96:60000"
    #     https_proxy = "https://188.211.26.96:60000"
    #
    #     proxyDict = {
    #         "http"  : http_proxy,
    #         "https" : https_proxy
    #     }
    #     reqeust.meta['proxies'] = proxyDict
    #     return reqeust.meta



    def parse(self, response):
        # print 'inside parse////////////////\t\t\t\t\t',response.url

        doc = html.fromstring(response.body)
        meta = {'handle_httpstatus_list': [302], 'dont_redirect': True}

        # my_session = emp.Session(base_url = 'https://stackoverflow.com')
        # my_session.set_proxy('127.0.0.1', 9000)
        # request = Request(url="https://www.linkedin.com/")
        # request.meta['proxy'] = "http://188.211.26.96:60000"

        # proxy_support = urllib2.ProxyHandler({"http":"http://188.211.26.96:60000"})
        # opener = urllib2.build_opener(proxy_support)
        # urllib2.install_opener(opener)
        #
        # htmls = urllib2.urlopen("https://www.linkedin.com/").read()
        # print "\n\n\n\n\n\n\n\n >>>>>>>>>>>>>>>>>>>>>",htmls
        # request = Request(url="https://www.linkedin.com/")
        # request.meta['proxy'] = "http://188.211.26.43:60000"
        # meta = self.process_reqeust(request)
        csrftoken = doc.xpath('.//input[@name="csrfToken"]/@value')
        return [FormRequest.from_response(response,
                        formdata={'session_key': 'tharaseb10@gmail.com', 'session_password': 'heera1234',
                                  'csrfToken': csrftoken},
                        callback=self.after_login, formxpath='.//form[@class="login-form"]', meta=meta)]
    # def check_ip(self, response):
    #    pub_ip = response.xpath('//body/text()').re('\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}')[0]
    #    print "My public IP is: " + pub_ip




    def after_login(self, response):
        meta = response.meta.copy()
        # request = Request(url="http://checkip.dyndns.org/")
        # meta = self.process_reqeust(request)
        # yield Request('http://checkip.dyndns.org/', callback=self.check_ip, meta=meta)
        # yield other requests from start_urls here if needed


        # meta = response.meta.copy()
        # doc = html.fromstring(response.body)



        # def check_ip(self, response):
        # pub_ip = doc.xpath('//body/text()').re('\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}')[0]
        # print "\n\n\n\n\n My public IP is: " + pub_ip
        #
        f = resource_stream('emp', 'companies1.csv')
        SIGNED_KEY = f.read()
        single_company_list = SIGNED_KEY.split('\n')
        for company in single_company_list:
          if company:
            company_name = company.split(',')[1]
            company_city = company.split(',')[6]
            state = company.split(',')[8]
            company_street = company.split(',')[5]
            company_zip = company.split(',')[9]
            meta['company_name'] = company_name
            meta['company_city'] = company_city
            meta['company_street'] = company_street
            meta['company_zip'] = company_zip
            #
            # meta = {'company_name': company_name, 'company_city': company_city,'company_street': company_street,
            #         'company_zip': company_zip }
            new_name = company_name.replace(' ', '+')
            company_name_url = new_name.replace('&', '%26')
            name = self.company_search_url.format(company_name=company_name_url)
            yield Request(url=name, callback=self.parse_companies, meta=meta, dont_filter=True)


    def parse_companies(self, response):
        # print'BODY......\n\n\n',response.body
        self.excecute_random_event_matrix()
        meta = response.meta.copy()


        # print ">>>>>>>>>>>>>inside parse_companies", response.url
        company_list_json = re.search(r';\"><!--(.+)--></code>',response.body, re.DOTALL)
        # print ">>>>>>>company_list_json", company_list_json
        if company_list_json:
            data = json.loads(company_list_json.group(1))
            # print 'ayooooooooooooooooo\t\t\t\t',data
            company_list = data['content']['page']['voltron_unified_search_json']['search']['results']


            # print "company_listtttttttttttttttttttttttt", company_list
            if len(company_list) > 1:
              for company in company_list:
                  self.ss = self.ss+1
                  company_name = company['company']['fmt_canonicalName']


                  # print "\n\n\n\n\n\n\ \t\t\t\t\t\t********************", company_name
                  nn = company_name.replace('<B>','')
                  nnn = nn.replace('</B>','')
                  if nnn.lower() == meta['company_name'].lower():
                    company_url = company['company']['link_biz_overview_6']
                    # meta['company_name'] = company_name
                    time.sleep(6)
                    yield Request(url=self.domain+company_url, callback=self.parse_single_company, meta=meta, dont_filter=True)

                  else:
                    with open('new_linkedin_unmatched_list.csv', 'a') as csvfile3:
                      fieldnames = [meta['company_name']]
                      writer = csv.DictWriter(csvfile3, fieldnames=fieldnames)
                      writer.writeheader()

            elif len(company_list) == 1:
              company_url = company_list[0]['company']['link_biz_overview_6']
              yield Request(url=self.domain+company_url, callback=self.parse_single_company, meta=meta, dont_filter=True)



    def parse_single_company(self, response):
        self.excecute_random_event_matrix()
        doc = html.fromstring(response.body)
        meta = response.meta.copy()
        street = doc.xpath('.//span[@class="street-address"]/text()')
        city = doc.xpath('.//span[@class="locality"]/text()')
        zip_code = doc.xpath('.//span[@class="postal-code"]/text()')
        flag=0
        if street:
          street = self._remove_special_characters(street)
          if street.lower().strip() == meta['company_street'].lower().strip():
              flag = 1
          else:
            if city:
              city = self._remove_special_characters(city)
              if city.lower().strip() == meta['company_city'].lower().strip():
                flag = 1
        elif city:
          city = self._remove_special_characters(city)
          if city.lower().strip() == meta['company_city'].lower().strip():
            flag = 1
        elif zip_code:
          if zip_code[0].split('-')[0] == meta['company_zip']:
            flag = 1

        if flag == 1:
          employee_page_url = re.search(r'Employees on LinkedIn(.+)See all',response.body, re.DOTALL)
          if employee_page_url:
              company_employee_listing_url = re.search(r'href=\"(.+)">',employee_page_url.group(1), re.DOTALL)
              if company_employee_listing_url:
                  time.sleep(6)
                  yield Request(url='https://www.linkedin.com'+company_employee_listing_url.group(1), meta=meta,
                              callback=self.parse_employee_listing_page)

        if flag == 0:
          fieldnames = [meta['company_name']]
          with open('new_linkedin_unmatched_list.csv', 'a') as csvfile3:
            writer = csv.DictWriter(csvfile3, fieldnames=fieldnames)
            writer.writeheader()


    def parse_employee_listing_page(self, response):
        self.excecute_random_event_matrix()
        meta = response.meta.copy()

        print ">>>>>>>>>>",response.body
        employee_list_json = re.search(r'\"results\"\:(.+)\,\"i18n_looking_for_someone\"',response.body, re.DOTALL)
        if employee_list_json:
          # ss = re.sub('\"distanceP\":\\u002d1\,',' ',employee_list_json.group(1))
          ss = employee_list_json.group(1).replace('\u002d1', '""', 20)
          datas = json.loads(ss)
          for employee in datas:
              if 'person' in employee:
                  if 'link_nprofile_view_4' in employee['person']:
                      employee_url = employee['person']['link_nprofile_view_4']
                      time.sleep(10)
                      yield Request(url=employee_url, callback=self.parse_employees, meta=meta)
                  elif 'link_nprofile_view_headless' in employee['person']:
                      employee_url = employee['person']['link_nprofile_view_headless']
                      time.sleep(7)

                      # request = Request(url=employee_url)
                      # request.meta['proxy'] = "http://188.211.26.43:60000"
                      yield Request(url=employee_url, callback=self.parse_employees, meta=meta)

        # if 'pagination_count' not in meta:
          # meta['pagination_count'] = 0
        if 'pagination' not  in meta:
          pagination = re.search(r'\"resultCount\"\:(.+),\"company_search_link\"',response.body)
          meta['url'] = response.url

          if pagination:
            meta['pagination_count'] = int(pagination.group(1))
            meta['page_no'] = 1

        if 'pagination_count' in meta:
          if meta['pagination_count'] > 10:
            pagination_count = meta['pagination_count']
            meta['pagination_count'] = pagination_count - 10
            meta['pagination'] = ""
            page_no = meta['page_no'] + 1
            meta['page_no'] = page_no
            pagination_url = meta['url']+'&page_num={page_no}&'
            new_employee_url = pagination_url.format(page_no=page_no)
            time.sleep(9)
            yield Request(url=new_employee_url, meta=meta, callback=self.parse_employee_listing_page)


    def parse_employees(self, response):
        self.excecute_random_event_matrix()
        self.i=self.i+1

        print "\n\n\n\n\n\nn..>>>>>", response, "\n\n\n\n\n................."
        doc = html.fromstring(response.body)
        meta = response.meta
        name = doc.xpath('.//span[@class="full-name"]/text()')
        item = items.EmployeeDetailsItem()
        designation = doc.xpath('.//p[@class="title"]/text()')
        designation = ','.join(designation)
        address = doc.xpath('.//div[@id="location"]//span[@class="locality"]/a/text()')
        address = ','.join(address)
        linkedin_id = doc.xpath('.//a[@class="view-public-profile"]/text()')
        linkedin_id = ','.join(linkedin_id)
        company_name = meta['company_name']
        item['designation'] = doc.xpath('.//p[@class="title"]/text()')
        item['employee_address'] = doc.xpath('.//div[@id="location"]//span[@class="locality"]/a/text()')
        item['linkedin_id'] = doc.xpath('.//a[@class="view-public-profile"]/text()')
        item['company_name'] = meta['company_name']
        item['given_street'] = meta['company_street']
        item['given_city'] = meta['company_city']
        meta['item'] = item
        if name != ['LinkedIn Member']:
          item['name'] = name
          fieldnames = [name[0], designation, address, linkedin_id, company_name,meta['company_street'],meta['company_city'] ]
          with open('new_linkedin_items.csv', 'a') as csvfile1:
            writer = csv.DictWriter(csvfile1, fieldnames=fieldnames)
            writer.writeheader()
          yield item
        else:
          # new_get_name_link = doc.xpath('.//div[@class="katy-button-group button-group-primary"]//a/@href')[0]
          new_link = 'https://www.linkedin.com/premium/inmail/compose?destID={name_id}&'
          emp_id = doc.xpath('.//button[@data-page-type="full_page"]/@data-page-tracking-info')[0]
          name_id_dict = json.loads(emp_id)
          name_id = name_id_dict['vid']
          get_name_link = new_link.format(name_id=name_id)
          meta['item'] = item
          meta['designation'] = designation
          meta['address'] = address
          meta['linkedin_id'] = linkedin_id
          yield Request(url=get_name_link, callback=self.parse_name, meta=meta)


    def parse_name(self, response):
        doc = html.fromstring(response.body)
        meta = response.meta
        name = doc.xpath('.//a[@id="recip-mp-name"]/text()')
        item = meta['item']
        item['name'] = name
        fieldnames = [name[0], meta['designation'], meta['address'], meta['linkedin_id'], meta['company_name'], meta['company_street'],meta['company_city']]
        with open('new_linkedin_items.csv', 'a') as csvfile1:
          writer = csv.DictWriter(csvfile1, fieldnames=fieldnames)
          writer.writeheader()
        yield item

    def _remove_special_characters(self, data):
        """
        Removes special characters from the inout data
        :param data: String
        :return: String
        """
        if ',' in data[0]:
          data = data[0].replace(',','')
          return data
        else:
          return data[0]



